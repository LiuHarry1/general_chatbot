"""
çŸ­æœŸè®°å¿†æ¨¡å—
è´Ÿè´£ç®¡ç†å¯¹è¯å†å²ã€å‹ç¼©å’Œæœ€è¿‘çš„ä¸Šä¸‹æ–‡
"""
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
from collections import deque
import threading
import uuid

from utils.logger import app_logger, log_execution_time
from memory.redis_manager import redis_manager

logger = logging.getLogger(__name__)


class ShortTermMemory:
    """çŸ­æœŸè®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self.redis_manager = redis_manager
        
        # çŸ­æœŸè®°å¿†é…ç½®
        self.max_tokens = 3000  # 3k tokené˜ˆå€¼
        self.warning_tokens = 2500  # è­¦å‘Šé˜ˆå€¼
        self.max_recent_turns = 3  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
        
        # å¼‚æ­¥å‹ç¼©é…ç½®
        self.compression_queue = deque()  # å‹ç¼©ä»»åŠ¡é˜Ÿåˆ—
        self.compression_lock = threading.Lock()  # é˜Ÿåˆ—é”
        self.max_concurrent_compressions = 3  # æœ€å¤§å¹¶å‘å‹ç¼©æ•°
        self.max_queue_size = 100  # æœ€å¤§é˜Ÿåˆ—å¤§å°
        self.active_compressions = 0  # å½“å‰æ´»è·ƒå‹ç¼©æ•°
        
        # é€’å¢æ€»ç»“é…ç½®
        self.summary_layers = {
            'L1': {'max_turns': 2, 'description': 'å•è½®å¯¹è¯æ‘˜è¦'},  # æœ€è¿‘2è½®
            'L2': {'max_turns': 5, 'description': 'å¤šè½®å¯¹è¯æ‘˜è¦'},  # æœ€è¿‘5è½®
            'L3': {'max_turns': 10, 'description': 'ä¸»é¢˜èšç±»æ‘˜è¦'}   # æœ€è¿‘10è½®
        }
        
        # å¼‚æ­¥å‹ç¼©å¤„ç†å™¨å°†åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶å¯åŠ¨
        self._compression_processor_started = False
        
        app_logger.info(f"ShortTermMemory initialized - enabled: {enabled}")
    
    async def _ensure_compression_processor_started(self) -> None:
        """ç¡®ä¿å‹ç¼©å¤„ç†å™¨å·²å¯åŠ¨"""
        if not self._compression_processor_started and self.enabled:
            self._compression_processor_started = True
            asyncio.create_task(self._compression_processor())
            app_logger.info("Compression processor started")
    
    @log_execution_time(log_args=True)
    async def get_recent_context(
        self,
        user_id: str,
        conversation_id: str,
        limit: int = 5
    ) -> Dict[str, Any]:
        """è·å–æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼šRedisä¼˜å…ˆï¼ŒDBå›é€€"""
        if not self.enabled:
            app_logger.info(f"ğŸ” [SHORT-TERM] Memory disabled for {user_id}:{conversation_id}")
            return {
                "context": "",
                "metadata": {
                    "enabled": False,
                    "reason": "Short-term memory disabled"
                }
            }
        
        try:
            app_logger.info(f"ğŸ” [SHORT-TERM] Getting context for {user_id}:{conversation_id} (limit={limit})")
            
            # 1. ä¼˜å…ˆä»Redisè·å–çŸ­æœŸè®°å¿†
            redis_context = await self._get_from_redis(user_id, conversation_id, limit)
            conversations = await self.redis_manager.get_recent_conversations(user_id, conversation_id, limit)
            
            if redis_context:
                # Redisä¸­æœ‰æ•°æ®ï¼Œç›´æ¥è¿”å›
                app_logger.info(f"ğŸ“ [SHORT-TERM] Retrieved from Redis for {user_id}:{conversation_id}")
                app_logger.info(f"ğŸ“„ [SHORT-TERM] Context content: {redis_context[:200]}...")
                return {
                    "context": redis_context,
                    "metadata": {
                        "enabled": True,
                        "source": "redis",
                        "limit": limit,
                        "conversations": conversations  # åŒ…å«åŸå§‹å¯¹è¯æ•°æ®ç”¨äºæ„å›¾è¯†åˆ«
                    }
                }
            
            # 2. Redisä¸­æ²¡æœ‰æ•°æ®ï¼Œä»æ•°æ®åº“è·å–å¹¶å­˜å‚¨åˆ°Redis
            from database import conversation_repo
            
            recent_messages = conversation_repo.get_current_conversation_messages(
                conversation_id=conversation_id,
                limit=limit
            )
            
            app_logger.info(f"ğŸ“Š [SHORT-TERM] Retrieved {len(recent_messages)} messages from database for {user_id}:{conversation_id}")
            
            if not recent_messages:
                app_logger.info(f"ğŸ“­ [SHORT-TERM] No messages found for {user_id}:{conversation_id}")
                return {
                    "context": "",
                    "metadata": {
                        "enabled": True,
                        "source": "database",
                        "recent_turns": 0,
                        "limit": limit,
                        "conversations": []  # ç©ºåˆ—è¡¨ç”¨äºæ„å›¾è¯†åˆ«
                    }
                }
            
            # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
            total_tokens = self._count_tokens_for_messages(recent_messages)
            app_logger.info(f"ğŸ”¢ [SHORT-TERM] Token count: {total_tokens}/{self.max_tokens} for {user_id}:{conversation_id}")
            
            if total_tokens > self.max_tokens:
                # è¶…è¿‡3k tokenï¼Œéœ€è¦å‹ç¼©
                app_logger.info(f"ğŸ—œï¸ [SHORT-TERM] Compressing messages for {user_id}:{conversation_id}")
                await self._compress_and_store(user_id, conversation_id, recent_messages)
                
                # é‡æ–°ä»Redisè·å–å‹ç¼©åçš„æ•°æ®
                redis_context = await self._get_from_redis(user_id, conversation_id, limit)
                conversations = await self.redis_manager.get_recent_conversations(user_id, conversation_id, limit)
                app_logger.info(f"ğŸ“„ [SHORT-TERM] Compressed context: {redis_context[:200]}...")
                return {
                    "context": redis_context or "",
                    "metadata": {
                        "enabled": True,
                        "source": "redis_compressed",
                        "recent_turns": len(recent_messages),
                        "compressed": True,
                        "limit": limit,
                        "conversations": conversations  # åŒ…å«åŸå§‹å¯¹è¯æ•°æ®ç”¨äºæ„å›¾è¯†åˆ«
                    }
                }
            else:
                # æœªè¶…è¿‡3k tokenï¼Œç›´æ¥å­˜å‚¨åˆ°Redis
                app_logger.info(f"ğŸ’¾ [SHORT-TERM] Storing {len(recent_messages)} messages to Redis for {user_id}:{conversation_id}")
                for i, msg in enumerate(recent_messages):
                    await self.redis_manager.store_conversation(
                        user_id=user_id,
                        conversation_id=conversation_id,
                        message=msg.get('user_message', ''),
                        response=msg.get('ai_response', ''),
                        metadata={}
                    )
                    app_logger.info(f"ğŸ’¬ [SHORT-TERM] Message {i+1}: User: {msg.get('user_message', '')[:50]}... | AI: {msg.get('ai_response', '')[:50]}...")
                
                # æ ¼å¼åŒ–å¹¶è¿”å›
                context = self._format_recent_messages(recent_messages)
                conversations = await self.redis_manager.get_recent_conversations(user_id, conversation_id, limit)
                app_logger.info(f"ğŸ“„ [SHORT-TERM] Formatted context: {context[:200]}...")
                return {
                    "context": context,
                    "metadata": {
                        "enabled": True,
                        "conversations": conversations,  # åŒ…å«åŸå§‹å¯¹è¯æ•°æ®ç”¨äºæ„å›¾è¯†åˆ«
                        "source": "database_to_redis",
                        "recent_turns": len(recent_messages),
                        "compressed": False,
                        "limit": limit
                    }
                }
            
        except Exception as e:
            app_logger.error(f"Failed to get recent context: {e}")
            return {
                "context": "",
                "metadata": {
                    "enabled": True,
                    "error": str(e)
                }
            }
    
    def _count_tokens(self, messages: List[Dict[str, Any]]) -> int:
        """ä¼°ç®—æ¶ˆæ¯çš„tokenæ•°é‡"""
        total_text = ""
        for message in messages:
            total_text += message.get("content", "")
        
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡1ä¸ªå­—ç¬¦â‰ˆ1.5ä¸ªtokenï¼Œè‹±æ–‡1ä¸ªè¯â‰ˆ1ä¸ªtoken
        chinese_chars = len([c for c in total_text if '\u4e00' <= c <= '\u9fff'])
        english_words = len([w for w in total_text.split() if w.isalpha()])
        
        return int(chinese_chars * 1.5 + english_words)
    
    def _format_recent_messages(self, messages: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æœ€è¿‘çš„æ¶ˆæ¯"""
        if not messages:
            return ""
        
        formatted = []
        for msg in messages:
            user_msg = msg.get('user_message', '')
            ai_response = msg.get('ai_response', '')
            if user_msg and ai_response:
                formatted.append(f"User: {user_msg}")
                formatted.append(f"Assistant: {ai_response}")
                formatted.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(formatted)
    
    def _format_conversations(self, conversations: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²ï¼ˆå»é‡å¤„ç†ï¼‰"""
        if not conversations:
            return ""
        
        # å»é‡å¤„ç†ï¼šä½¿ç”¨æ¶ˆæ¯å†…å®¹ä½œä¸ºkey
        seen_messages = set()
        unique_conversations = []
        
        for conv in conversations:
            message = conv.get("message", "")
            response = conv.get("response", "")
            
            # åˆ›å»ºæ¶ˆæ¯çš„å”¯ä¸€æ ‡è¯†
            message_key = f"{message}|{response}"
            
            if message_key not in seen_messages:
                seen_messages.add(message_key)
                unique_conversations.append(conv)
        
        app_logger.info(f"ğŸ“Š [SHORT-TERM] Filtered {len(conversations)} -> {len(unique_conversations)} unique conversations")
        
        formatted = []
        for conv in unique_conversations:
            timestamp = conv.get("timestamp", "")
            message = conv.get("message", "")
            response = conv.get("response", "")
            
            formatted.append(f"[{timestamp}] ç”¨æˆ·: {message}")
            formatted.append(f"[{timestamp}] åŠ©æ‰‹: {response}")
        
        return "\n".join(formatted)
    
    async def store_conversation(
        self,
        user_id: str,
        conversation_id: str,
        message: str,
        response: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """å­˜å‚¨å•è½®å¯¹è¯åˆ°çŸ­æœŸè®°å¿†"""
        if not self.enabled:
            return False
        
        try:
            await self.redis_manager.store_conversation(
                user_id=user_id,
                conversation_id=conversation_id,
                message=message,
                response=response,
                metadata=metadata or {}
            )
            return True
            
        except Exception as e:
            app_logger.error(f"Failed to store conversation: {e}")
            return False
    
    async def smart_store_conversation(
        self,
        user_id: str,
        conversation_id: str,
        message: str,
        response: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """æ™ºèƒ½å­˜å‚¨å¯¹è¯ï¼šä½¿ç”¨å¼‚æ­¥å‹ç¼©å’Œé€’å¢æ€»ç»“"""
        if not self.enabled:
            app_logger.info(f"ğŸ” [SHORT-TERM] Memory disabled, skipping storage for {user_id}:{conversation_id}")
            return False
        
        try:
            app_logger.info(f"ğŸ’¾ [SHORT-TERM] Smart storing conversation for {user_id}:{conversation_id}")
            app_logger.info(f"ğŸ’¬ [SHORT-TERM] New message: User: {message[:100]}...")
            app_logger.info(f"ğŸ¤– [SHORT-TERM] New response: AI: {response[:100]}...")
            
            # ç¡®ä¿å‹ç¼©å¤„ç†å™¨å·²å¯åŠ¨
            await self._ensure_compression_processor_started()
            
            # å…ˆç›´æ¥å­˜å‚¨å½“å‰å¯¹è¯
            await self.redis_manager.store_conversation(
                user_id=user_id,
                conversation_id=conversation_id,
                message=message,
                response=response,
                metadata=metadata or {}
            )
            app_logger.info(f"âœ… [SHORT-TERM] Stored conversation to Redis for {user_id}:{conversation_id}")
            
            # è·å–å½“å‰å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯
            from database import conversation_repo
            all_messages = conversation_repo.get_current_conversation_messages(
                conversation_id=conversation_id,
                limit=100
            )
            
            # è®¡ç®—æ€»tokenæ•°
            total_tokens = self._count_tokens_for_messages(all_messages)
            app_logger.info(f"ğŸ”¢ [SHORT-TERM] Total messages: {len(all_messages)}, Total tokens: {total_tokens}/{self.max_tokens}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼‚æ­¥å‹ç¼©
            if total_tokens > self.warning_tokens:
                priority = 'high' if total_tokens > self.max_tokens else 'normal'
                app_logger.info(f"âš ï¸ [SHORT-TERM] Token limit exceeded, queuing compression task with priority: {priority}")
                # æ·»åŠ åˆ°å¼‚æ­¥å‹ç¼©é˜Ÿåˆ—
                await self._queue_compression_task(
                    user_id=user_id,
                    conversation_id=conversation_id,
                    priority=priority
                )
            else:
                app_logger.info(f"âœ… [SHORT-TERM] Token count within limits, no compression needed")
            
            return True
            
        except Exception as e:
            app_logger.error(f"âŒ [SHORT-TERM] Failed to smart store conversation: {e}")
            return False
    
    async def clear_user_data(self, user_id: str) -> bool:
        """æ¸…ç†ç”¨æˆ·çš„çŸ­æœŸè®°å¿†æ•°æ®"""
        if not self.enabled:
            return True
        
        try:
            await self.redis_manager.clear_user_data(user_id)
            return True
            
        except Exception as e:
            app_logger.error(f"Failed to clear user data: {e}")
            return False
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        if not self.enabled:
            return {
                "status": "disabled",
                "message": "Short-term memory is disabled",
                "components": {
                    "redis_manager": "disabled"
                }
            }
        
        try:
            # æ£€æŸ¥Redisè¿æ¥
            redis_health = await self.redis_manager.health_check()
            
            # è·å–å‹ç¼©é˜Ÿåˆ—çŠ¶æ€
            with self.compression_lock:
                queue_size = len(self.compression_queue)
                queue_tasks = [
                    {
                        'id': task.get('id', 'unknown'),
                        'user_id': task.get('user_id', 'unknown'),
                        'conversation_id': task.get('conversation_id', 'unknown'),
                        'priority': task.get('priority', 'normal'),
                        'status': task.get('status', 'queued'),
                        'created_at': task.get('created_at', 'unknown')
                    }
                    for task in list(self.compression_queue)
                ]
            
            return {
                "status": "ok",
                "message": "Short-term memory is healthy",
                "components": {
                    "redis_manager": redis_health["status"],
                    "async_compression": "enabled",
                    "incremental_summary": "enabled"
                },
                "config": {
                    "max_tokens": self.max_tokens,
                    "warning_tokens": self.warning_tokens,
                    "max_recent_turns": self.max_recent_turns,
                    "max_concurrent_compressions": self.max_concurrent_compressions,
                    "max_queue_size": self.max_queue_size,
                    "enabled": self.enabled
                },
                "compression_status": {
                    "queue_size": queue_size,
                    "active_compressions": self.active_compressions,
                    "queued_tasks": queue_tasks
                },
                "summary_layers": {
                    layer: {
                        "max_turns": config["max_turns"],
                        "description": config["description"]
                    }
                    for layer, config in self.summary_layers.items()
                }
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Short-term memory health check failed: {e}",
                "components": {
                    "redis_manager": "error"
                }
            }


    async def _get_summarized_context(
        self,
        user_id: str,
        conversation_id: str
    ) -> str:
        """è·å–æ›´æ—©å¯¹è¯çš„summarizedä¿¡æ¯"""
        try:
            # ä»Redisè·å–å½“å‰å¯¹è¯çš„summarizedä¿¡æ¯
            summary_key = f"conversation_summary:{user_id}:{conversation_id}"
            summary = self.redis_manager.redis_conn.get(summary_key)
            
            if summary:
                # Redisè¿”å›çš„æ•°æ®å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—èŠ‚ï¼Œéœ€è¦å¤„ç†
                if isinstance(summary, bytes):
                    return summary.decode('utf-8')
                return str(summary)
            
            return ""
            
        except Exception as e:
            app_logger.error(f"Failed to get summarized context: {e}")
            return ""
    
    async def store_conversation_summary(
        self,
        user_id: str,
        conversation_id: str,
        summary: str
    ) -> bool:
        """å­˜å‚¨å¯¹è¯æ‘˜è¦åˆ°Redis"""
        try:
            summary_key = f"conversation_summary:{user_id}:{conversation_id}"
            self.redis_manager.redis_conn.set(
                summary_key,
                summary,
                ex=86400 * 30  # 30å¤©è¿‡æœŸ
            )
            app_logger.info(f"Stored conversation summary for {user_id}:{conversation_id}")
            return True
            
        except Exception as e:
            app_logger.error(f"Failed to store conversation summary: {e}")
            return False
    
    async def generate_and_store_summary(
        self,
        user_id: str,
        conversation_id: str,
        messages: List[Dict[str, Any]]
    ) -> bool:
        """ç”Ÿæˆå¹¶å­˜å‚¨å¯¹è¯æ‘˜è¦"""
        try:
            if len(messages) < 6:  # å°‘äº3è½®å¯¹è¯ä¸éœ€è¦æ‘˜è¦
                return False
            
            # ä½¿ç”¨AIç”Ÿæˆæ‘˜è¦
            from services.ai_service import ai_service
            
            # æ„å»ºå¯¹è¯æ–‡æœ¬
            conversation_text = ""
            for msg in messages[:-4]:  # æ’é™¤æœ€è¿‘2è½®å¯¹è¯
                role = msg.get('role', '')
                content = msg.get('content', '')
                if role == 'user':
                    conversation_text += f"ç”¨æˆ·: {content}\n"
                elif role == 'assistant':
                    conversation_text += f"åŠ©æ‰‹: {content}\n"
            
            if not conversation_text.strip():
                return False
            
            # ç”Ÿæˆæ‘˜è¦
            summary_prompt = f"""è¯·ä¸ºä»¥ä¸‹å¯¹è¯ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ï¼š

{conversation_text}

æ‘˜è¦è¦æ±‚ï¼š
1. ç”¨2-3å¥è¯æ¦‚æ‹¬å¯¹è¯çš„ä¸»è¦å†…å®¹
2. ä¿ç•™é‡è¦çš„ç”¨æˆ·éœ€æ±‚å’ŒAIçš„å›ç­”è¦ç‚¹
3. ä½¿ç”¨ä¸­æ–‡ï¼Œè¯­è¨€ç®€æ´æ˜äº†

æ‘˜è¦ï¼š"""
            
            summary = await ai_service.generate_response(
                user_message=summary_prompt,
                intent="normal",
                full_context=""
            )
            
            if summary and len(summary.strip()) > 10:
                return await self.store_conversation_summary(user_id, conversation_id, summary.strip())
            
            return False
            
        except Exception as e:
            app_logger.error(f"Failed to generate and store summary: {e}")
            return False
    
    def _count_tokens_for_messages(self, messages: List[Dict[str, Any]]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„tokenæ•°é‡"""
        total_text = ""
        for msg in messages:
            total_text += msg.get('user_message', '') + " " + msg.get('ai_response', '')
        
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡1ä¸ªå­—ç¬¦â‰ˆ1.5ä¸ªtokenï¼Œè‹±æ–‡1ä¸ªè¯â‰ˆ1ä¸ªtoken
        chinese_chars = len([c for c in total_text if '\u4e00' <= c <= '\u9fff'])
        english_words = len([w for w in total_text.split() if w.isalpha()])
        
        return int(chinese_chars * 1.5 + english_words)
    
    async def _compress_and_store(
        self,
        user_id: str,
        conversation_id: str,
        messages: List[Dict[str, Any]]
    ) -> bool:
        """å‹ç¼©å¹¶å­˜å‚¨å¯¹è¯"""
        try:
            # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
            recent_messages = messages[-self.max_recent_turns:] if len(messages) > self.max_recent_turns else messages
            old_messages = messages[:-self.max_recent_turns] if len(messages) > self.max_recent_turns else []
            
            if not old_messages:
                # æ²¡æœ‰æ—§æ¶ˆæ¯éœ€è¦å‹ç¼©ï¼Œç›´æ¥å­˜å‚¨æœ€è¿‘å¯¹è¯
                for msg in recent_messages:
                    await self.redis_manager.store_conversation(
                        user_id=user_id,
                        conversation_id=conversation_id,
                        message=msg.get('user_message', ''),
                        response=msg.get('ai_response', ''),
                        metadata={}
                    )
                return True
            
            # ç”Ÿæˆæ—§æ¶ˆæ¯çš„æ‘˜è¦
            summary = await self._generate_summary_for_messages(old_messages)
            
            if summary:
                # å­˜å‚¨æ‘˜è¦åˆ°Redis
                await self.store_conversation_summary(user_id, conversation_id, summary)
                
                # å­˜å‚¨æœ€è¿‘3è½®å¯¹è¯
                for msg in recent_messages:
                    await self.redis_manager.store_conversation(
                        user_id=user_id,
                        conversation_id=conversation_id,
                        message=msg.get('user_message', ''),
                        response=msg.get('ai_response', ''),
                        metadata={}
                    )
                
                app_logger.info(f"Compressed and stored conversation for {user_id}:{conversation_id}")
                return True
            
            return False
            
        except Exception as e:
            app_logger.error(f"Failed to compress and store: {e}")
            return False
    
    async def _generate_summary_for_messages(self, messages: List[Dict[str, Any]]) -> str:
        """ä¸ºæ¶ˆæ¯åˆ—è¡¨ç”Ÿæˆæ‘˜è¦"""
        try:
            if not messages:
                return ""
            
            # æ„å»ºå¯¹è¯æ–‡æœ¬
            conversation_text = ""
            for msg in messages:
                conversation_text += f"ç”¨æˆ·: {msg.get('user_message', '')}\n"
                conversation_text += f"åŠ©æ‰‹: {msg.get('ai_response', '')}\n"
            
            if not conversation_text.strip():
                return ""
            
            # ä½¿ç”¨AIç”Ÿæˆæ‘˜è¦
            from services.ai_service import ai_service
            
            summary_prompt = f"""è¯·ä¸ºä»¥ä¸‹å¯¹è¯ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ï¼š

{conversation_text}

æ‘˜è¦è¦æ±‚ï¼š
1. ç”¨2-3å¥è¯æ¦‚æ‹¬å¯¹è¯çš„ä¸»è¦å†…å®¹
2. ä¿ç•™é‡è¦çš„ç”¨æˆ·éœ€æ±‚å’ŒAIçš„å›ç­”è¦ç‚¹
3. ä½¿ç”¨ä¸­æ–‡ï¼Œè¯­è¨€ç®€æ´æ˜äº†

æ‘˜è¦ï¼š"""
            
            summary = await ai_service.generate_response(
                user_message=summary_prompt,
                intent="normal",
                full_context=""
            )
            return summary.strip() if summary else ""
            
        except Exception as e:
            app_logger.error(f"Failed to generate summary for messages: {e}")
            return ""
    
    async def _get_from_redis(self, user_id: str, conversation_id: str, limit: int = 10) -> str:
        """ä»Redisè·å–çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒåˆ†å±‚æ‘˜è¦ï¼‰"""
        try:
            app_logger.info(f"ğŸ” [SHORT-TERM] Getting context from Redis for {user_id}:{conversation_id}")
            
            # è·å–Redisä¸­çš„å¯¹è¯æ•°æ®
            conversations = await self.redis_manager.get_recent_conversations(
                user_id=user_id,
                conversation_id=conversation_id,
                limit=limit
            )
            app_logger.info(f"ğŸ“Š [SHORT-TERM] Retrieved {len(conversations)} conversations from Redis")
            
            # è·å–åˆ†å±‚æ‘˜è¦
            layer_summaries = await self._get_layer_summaries(user_id, conversation_id)
            app_logger.info(f"ğŸ“‹ [SHORT-TERM] Layer summaries: {list(layer_summaries.keys())}")
            
            # æ‰“å°æ¯å±‚æ‘˜è¦å†…å®¹
            for layer, summary in layer_summaries.items():
                app_logger.info(f"ğŸ“„ [SHORT-TERM] {layer} Summary: {summary[:100]}...")
            
            # ç»„åˆä¸Šä¸‹æ–‡
            context_parts = []
            
            # æ·»åŠ åˆ†å±‚æ‘˜è¦ï¼ˆä»æœ€æ—§åˆ°æœ€æ–°ï¼‰
            if layer_summaries.get('L3'):
                context_parts.append(f"Earlier conversation summary (L3):\n{layer_summaries['L3']}")
                app_logger.info(f"ğŸ“ [SHORT-TERM] Added L3 summary to context")
            
            if layer_summaries.get('L2'):
                context_parts.append(f"Recent conversation summary (L2):\n{layer_summaries['L2']}")
                app_logger.info(f"ğŸ“ [SHORT-TERM] Added L2 summary to context")
            
            if layer_summaries.get('L1'):
                context_parts.append(f"Latest conversation summary (L1):\n{layer_summaries['L1']}")
                app_logger.info(f"ğŸ“ [SHORT-TERM] Added L1 summary to context")
            
            # æ·»åŠ æœ€è¿‘çš„å¯¹è¯
            if conversations:
                recent_context = self._format_conversations(conversations)
                context_parts.append(f"Current conversation:\n{recent_context}")
                app_logger.info(f"ğŸ“ [SHORT-TERM] Added {len(conversations)} recent conversations to context")
            
            final_context = "\n\n".join(context_parts)
            app_logger.info(f"ğŸ“„ [SHORT-TERM] Final context length: {len(final_context)} characters")
            
            return final_context
            
        except Exception as e:
            app_logger.error(f"âŒ [SHORT-TERM] Failed to get from Redis: {e}")
            return ""

    async def _get_layer_summaries(
        self,
        user_id: str,
        conversation_id: str
    ) -> Dict[str, str]:
        """è·å–åˆ†å±‚æ‘˜è¦"""
        try:
            summaries = {}
            for layer in ['L1', 'L2', 'L3']:
                summary_key = f"conversation_summary:{user_id}:{conversation_id}:{layer}"
                # ä½¿ç”¨å¼‚æ­¥æ–¹å¼æ‰§è¡ŒRedisæ“ä½œ
                summary = await asyncio.get_event_loop().run_in_executor(
                    None, self.redis_manager.redis_conn.get, summary_key
                )
                if summary:
                    if isinstance(summary, bytes):
                        summaries[layer] = summary.decode('utf-8')
                    else:
                        summaries[layer] = str(summary)
            return summaries
        except Exception as e:
            app_logger.error(f"Failed to get layer summaries: {e}")
            return {}

    async def _queue_compression_task(
        self,
        user_id: str,
        conversation_id: str,
        priority: str = 'normal'
    ) -> None:
        """å°†å‹ç¼©ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—"""
        try:
            task = {
                'id': str(uuid.uuid4()),
                'user_id': user_id,
                'conversation_id': conversation_id,
                'priority': priority,
                'created_at': datetime.now().isoformat(),
                'status': 'queued'
            }
            
            with self.compression_lock:
                # æ£€æŸ¥é˜Ÿåˆ—å¤§å°é™åˆ¶
                if len(self.compression_queue) >= self.max_queue_size:
                    # é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒæœ€è€çš„ä»»åŠ¡
                    if priority == 'high':
                        # é«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼Œä¸¢å¼ƒæœ€è€çš„æ™®é€šä¼˜å…ˆçº§ä»»åŠ¡
                        old_task = None
                        for i, queued_task in enumerate(self.compression_queue):
                            if queued_task.get('priority') == 'normal':
                                old_task = self.compression_queue.pop(i)
                                break
                        if old_task:
                            app_logger.warning(f"Queue full, discarded normal priority task {old_task.get('id', 'unknown')}")
                        else:
                            app_logger.warning(f"Queue full, cannot add high priority task {task['id']}")
                            return
                    else:
                        # æ™®é€šä¼˜å…ˆçº§ä»»åŠ¡ï¼Œä¸¢å¼ƒæœ€è€çš„ä»»åŠ¡
                        old_task = self.compression_queue.popleft()
                        app_logger.warning(f"Queue full, discarded task {old_task.get('id', 'unknown')}")
                
                # æ·»åŠ æ–°ä»»åŠ¡
                if priority == 'high':
                    # é«˜ä¼˜å…ˆçº§ä»»åŠ¡æ’å…¥åˆ°é˜Ÿåˆ—å‰é¢
                    self.compression_queue.appendleft(task)
                else:
                    # æ™®é€šä¼˜å…ˆçº§ä»»åŠ¡æ’å…¥åˆ°é˜Ÿåˆ—åé¢
                    self.compression_queue.append(task)
            
            app_logger.info(f"Queued compression task for {user_id}:{conversation_id} with priority {priority}")
            
        except Exception as e:
            app_logger.error(f"Failed to queue compression task: {e}")

    async def _compression_processor(self) -> None:
        """å¼‚æ­¥å‹ç¼©å¤„ç†å™¨"""
        while self.enabled:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒå‹ç¼©ä»»åŠ¡å’Œé˜Ÿåˆ—ä»»åŠ¡
                if self.active_compressions < self.max_concurrent_compressions and self.compression_queue:
                    # è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡
                    with self.compression_lock:
                        if not self.compression_queue:
                            continue
                        task = self.compression_queue.popleft()
                    
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    task['status'] = 'processing'
                    
                    # çº¿ç¨‹å®‰å…¨åœ°å¢åŠ æ´»è·ƒå‹ç¼©è®¡æ•°
                    with self.compression_lock:
                        self.active_compressions += 1
                    
                    # å¼‚æ­¥å¤„ç†å‹ç¼©ä»»åŠ¡
                    asyncio.create_task(self._process_compression_task(task))
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
                await asyncio.sleep(1)
                
            except Exception as e:
                app_logger.error(f"Compression processor error: {e}")
                await asyncio.sleep(5)

    async def _process_compression_task(self, task: Dict[str, Any]) -> None:
        """å¤„ç†å•ä¸ªå‹ç¼©ä»»åŠ¡"""
        try:
            user_id = task['user_id']
            conversation_id = task['conversation_id']
            task_id = task['id']
            
            app_logger.info(f"Processing compression task {task_id} for {user_id}:{conversation_id}")
            
            # è·å–å¯¹è¯æ¶ˆæ¯
            from database import conversation_repo
            all_messages = conversation_repo.get_current_conversation_messages(
                conversation_id=conversation_id,
                limit=100
            )
            
            if not all_messages:
                return
            
            # æ‰§è¡Œé€’å¢æ€»ç»“å‹ç¼©
            await self._incremental_compression(user_id, conversation_id, all_messages)
            
            app_logger.info(f"Completed compression task {task_id} for {user_id}:{conversation_id}")
            
        except Exception as e:
            app_logger.error(f"Failed to process compression task {task.get('id', 'unknown')}: {e}")
        finally:
            # çº¿ç¨‹å®‰å…¨åœ°å‡å°‘æ´»è·ƒå‹ç¼©è®¡æ•°
            with self.compression_lock:
                self.active_compressions = max(0, self.active_compressions - 1)

    async def _incremental_compression(
        self,
        user_id: str,
        conversation_id: str,
        messages: List[Dict[str, Any]]
    ) -> bool:
        """é€’å¢æ€»ç»“å‹ç¼©"""
        try:
            total_messages = len(messages)
            if total_messages < 6:  # å°‘äº3è½®å¯¹è¯ä¸éœ€è¦å‹ç¼©
                return True
            
            # è·å–ç°æœ‰çš„æ‘˜è¦å±‚
            existing_summaries = await self._get_existing_summaries(user_id, conversation_id)
            
            # åˆ†å±‚å¤„ç† - æŒ‰å±‚çº§ä»å¤§åˆ°å°å¤„ç†
            new_summaries = {}
            messages_to_keep = []
            
            # ç¡®å®šéœ€è¦ä¿ç•™çš„æ¶ˆæ¯æ•°é‡ï¼ˆå–æœ€å¤§çš„å±‚ï¼‰
            max_keep_turns = max(
                self.summary_layers['L1']['max_turns'],
                self.summary_layers['L2']['max_turns'], 
                self.summary_layers['L3']['max_turns']
            )
            
            # ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
            if total_messages > max_keep_turns:
                messages_to_keep = messages[-max_keep_turns:]
                messages_to_summarize = messages[:-max_keep_turns]
            else:
                messages_to_keep = messages
                messages_to_summarize = []
            
            # å¦‚æœæ²¡æœ‰éœ€è¦æ‘˜è¦çš„æ¶ˆæ¯ï¼Œç›´æ¥è¿”å›
            if not messages_to_summarize:
                app_logger.info(f"No messages need summarization for {user_id}:{conversation_id}")
                return True
            
            # æ ¹æ®æ¶ˆæ¯æ•°é‡å†³å®šç”Ÿæˆå“ªäº›å±‚çš„æ‘˜è¦
            if len(messages_to_summarize) >= 8:  # è¶³å¤Ÿç”ŸæˆL3æ‘˜è¦
                l3_summary = await self._generate_layer_summary(
                    'L3', messages_to_summarize, existing_summaries.get('L3')
                )
                if l3_summary:
                    new_summaries['L3'] = l3_summary
                
                # ç”ŸæˆL2æ‘˜è¦ï¼ˆä»L3æ‘˜è¦ä¸­æå–æˆ–é‡æ–°ç”Ÿæˆï¼‰
                l2_summary = await self._generate_layer_summary(
                    'L2', messages_to_summarize[-5:], existing_summaries.get('L2')
                )
                if l2_summary:
                    new_summaries['L2'] = l2_summary
                    
            elif len(messages_to_summarize) >= 3:  # è¶³å¤Ÿç”ŸæˆL2æ‘˜è¦
                l2_summary = await self._generate_layer_summary(
                    'L2', messages_to_summarize, existing_summaries.get('L2')
                )
                if l2_summary:
                    new_summaries['L2'] = l2_summary
            
            # æ€»æ˜¯ç”ŸæˆL1æ‘˜è¦ï¼ˆå¦‚æœæœ‰å¤šä½™æ¶ˆæ¯ï¼‰
            if len(messages_to_summarize) >= 1:
                l1_summary = await self._generate_layer_summary(
                    'L1', messages_to_summarize[-2:], existing_summaries.get('L1')
                )
                if l1_summary:
                    new_summaries['L1'] = l1_summary
            
            # å­˜å‚¨æ–°çš„æ‘˜è¦
            if new_summaries:
                await self._store_layer_summaries(user_id, conversation_id, new_summaries)
            
            # æ¸…ç†Redisä¸­çš„æ—§æ¶ˆæ¯ï¼Œåªä¿ç•™éœ€è¦ä¿ç•™çš„æ¶ˆæ¯
            await self._cleanup_old_messages(user_id, conversation_id, messages_to_keep)
            
            app_logger.info(f"Incremental compression completed for {user_id}:{conversation_id} - kept {len(messages_to_keep)} messages, generated {len(new_summaries)} summaries")
            return True
            
        except Exception as e:
            app_logger.error(f"Failed to perform incremental compression: {e}")
            return False

    async def _get_existing_summaries(
        self,
        user_id: str,
        conversation_id: str
    ) -> Dict[str, str]:
        """è·å–ç°æœ‰çš„æ‘˜è¦å±‚"""
        try:
            summaries = {}
            for layer in ['L1', 'L2', 'L3']:
                summary_key = f"conversation_summary:{user_id}:{conversation_id}:{layer}"
                # ä½¿ç”¨å¼‚æ­¥æ–¹å¼æ‰§è¡ŒRedisæ“ä½œ
                summary = await asyncio.get_event_loop().run_in_executor(
                    None, self.redis_manager.redis_conn.get, summary_key
                )
                if summary:
                    if isinstance(summary, bytes):
                        summaries[layer] = summary.decode('utf-8')
                    else:
                        summaries[layer] = str(summary)
            return summaries
        except Exception as e:
            app_logger.error(f"Failed to get existing summaries: {e}")
            return {}

    async def _generate_layer_summary(
        self,
        layer: str,
        messages: List[Dict[str, Any]],
        existing_summary: Optional[str] = None
    ) -> Optional[str]:
        """ç”ŸæˆæŒ‡å®šå±‚çš„æ‘˜è¦"""
        try:
            if not messages:
                return None
            
            # æ„å»ºå¯¹è¯æ–‡æœ¬
            conversation_text = ""
            for msg in messages:
                conversation_text += f"ç”¨æˆ·: {msg.get('user_message', '')}\n"
                conversation_text += f"åŠ©æ‰‹: {msg.get('ai_response', '')}\n"
            
            if not conversation_text.strip():
                return None
            
            # å¦‚æœæœ‰ç°æœ‰æ‘˜è¦ï¼Œè¿›è¡Œé€’å¢æ›´æ–°
            if existing_summary:
                summary_prompt = f"""è¯·åŸºäºç°æœ‰æ‘˜è¦å’Œæ–°å¯¹è¯å†…å®¹ï¼Œç”Ÿæˆæ›´æ–°çš„æ‘˜è¦ï¼š

ç°æœ‰æ‘˜è¦ï¼š
{existing_summary}

æ–°å¯¹è¯å†…å®¹ï¼š
{conversation_text}

è¦æ±‚ï¼š
1. ä¿ç•™ç°æœ‰æ‘˜è¦ä¸­çš„é‡è¦ä¿¡æ¯
2. æ•´åˆæ–°å¯¹è¯çš„å…³é”®å†…å®¹
3. ä¿æŒæ‘˜è¦ç®€æ´æ˜äº†ï¼ˆ{self.summary_layers[layer]['description']}ï¼‰
4. ä½¿ç”¨ä¸­æ–‡

æ›´æ–°åçš„æ‘˜è¦ï¼š"""
            else:
                summary_prompt = f"""è¯·ä¸ºä»¥ä¸‹å¯¹è¯ç”Ÿæˆæ‘˜è¦ï¼š

{conversation_text}

è¦æ±‚ï¼š
1. ç”¨2-3å¥è¯æ¦‚æ‹¬å¯¹è¯çš„ä¸»è¦å†…å®¹
2. ä¿ç•™é‡è¦çš„ç”¨æˆ·éœ€æ±‚å’ŒAIçš„å›ç­”è¦ç‚¹
3. ä½¿ç”¨ä¸­æ–‡ï¼Œè¯­è¨€ç®€æ´æ˜äº†
4. è¿™æ˜¯{self.summary_layers[layer]['description']}

æ‘˜è¦ï¼š"""
            
            # ä½¿ç”¨AIç”Ÿæˆæ‘˜è¦
            from services.ai_service import ai_service
            summary = await ai_service.generate_response(
                user_message=summary_prompt,
                intent="normal",
                full_context=""
            )
            
            return summary.strip() if summary else None
            
        except Exception as e:
            app_logger.error(f"Failed to generate {layer} summary: {e}")
            return None

    async def _store_layer_summaries(
        self,
        user_id: str,
        conversation_id: str,
        summaries: Dict[str, str]
    ) -> None:
        """å­˜å‚¨åˆ†å±‚æ‘˜è¦"""
        try:
            for layer, summary in summaries.items():
                if summary:
                    summary_key = f"conversation_summary:{user_id}:{conversation_id}:{layer}"
                    # ä½¿ç”¨å¼‚æ­¥æ–¹å¼æ‰§è¡ŒRedisæ“ä½œ
                    await asyncio.get_event_loop().run_in_executor(
                        None,
                        lambda: self.redis_manager.redis_conn.setex(
                            summary_key,
                            86400 * 30,  # 30å¤©è¿‡æœŸ
                            summary
                        )
                    )
                    app_logger.info(f"Stored {layer} summary for {user_id}:{conversation_id}")
        except Exception as e:
            app_logger.error(f"Failed to store layer summaries: {e}")

    async def _cleanup_old_messages(
        self,
        user_id: str,
        conversation_id: str,
        keep_messages: List[Dict[str, Any]]
    ) -> None:
        """æ¸…ç†Redisä¸­çš„æ—§æ¶ˆæ¯"""
        try:
            # è·å–Redisä¸­çš„æ‰€æœ‰å¯¹è¯
            conversations = await self.redis_manager.get_recent_conversations(
                user_id=user_id,
                conversation_id=conversation_id,
                limit=100
            )
            
            if not conversations:
                return
            
            # è·å–éœ€è¦ä¿ç•™çš„æ¶ˆæ¯IDé›†åˆ
            keep_message_ids = set()
            for msg in keep_messages:
                # å‡è®¾æ¶ˆæ¯æœ‰IDå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å†…å®¹hash
                msg_id = msg.get('id') or msg.get('message_id')
                if not msg_id:
                    # ä½¿ç”¨å†…å®¹ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                    content = f"{msg.get('user_message', '')}{msg.get('ai_response', '')}"
                    msg_id = str(hash(content))
                keep_message_ids.add(str(msg_id))
            
            # æ¸…ç†Redisä¸­ä¸åœ¨ä¿ç•™åˆ—è¡¨ä¸­çš„æ¶ˆæ¯
            deleted_count = 0
            for conv in conversations:
                conv_id = conv.get('id') or conv.get('message_id')
                if not conv_id:
                    content = f"{conv.get('message', '')}{conv.get('response', '')}"
                    conv_id = str(hash(content))
                
                if str(conv_id) not in keep_message_ids:
                    # åˆ é™¤æ—§æ¶ˆæ¯ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®Redisçš„å…·ä½“é”®ç»“æ„æ¥å®ç°ï¼‰
                    # æš‚æ—¶ä½¿ç”¨é€šç”¨çš„æ¸…ç†æ–¹æ³•
                    try:
                        # å‡è®¾Redisä¸­çš„é”®æ ¼å¼ä¸º: conversation:{user_id}:{conversation_id}:{message_id}
                        message_key = f"conversation:{user_id}:{conversation_id}:{conv_id}"
                        await asyncio.get_event_loop().run_in_executor(
                            None, self.redis_manager.redis_conn.delete, message_key
                        )
                        deleted_count += 1
                    except Exception as delete_error:
                        app_logger.warning(f"Failed to delete message {conv_id}: {delete_error}")
            
            app_logger.info(f"Cleanup completed for {user_id}:{conversation_id} - kept {len(keep_messages)} messages, deleted {deleted_count} old messages")
            
        except Exception as e:
            app_logger.error(f"Failed to cleanup old messages: {e}")

# å…¨å±€å®ä¾‹ - é»˜è®¤å¯ç”¨
short_term_memory = ShortTermMemory(enabled=True)
