"""
è®°å¿†å‹ç¼©å™¨
è´Ÿè´£å¼‚æ­¥å‹ç¼©å’Œé€’å¢æ€»ç»“
"""
import asyncio
import uuid
from typing import List, Dict, Any
from datetime import datetime
from collections import deque
import threading
import logging

from utils.logger import app_logger
from memory.redis_manager import redis_manager
from memory.summary_generator import summary_generator

logger = logging.getLogger(__name__)


class MemoryCompressor:
    """è®°å¿†å‹ç¼©å™¨ - è´Ÿè´£å¼‚æ­¥å‹ç¼©å’Œé€’å¢æ€»ç»“"""
    
    def __init__(self):
        # å¼‚æ­¥å‹ç¼©é…ç½®
        self.compression_queue = deque()  # å‹ç¼©ä»»åŠ¡é˜Ÿåˆ—
        self.compression_lock = threading.Lock()  # é˜Ÿåˆ—é”
        self.max_concurrent_compressions = 3  # æœ€å¤§å¹¶å‘å‹ç¼©æ•°
        self.max_queue_size = 100  # æœ€å¤§é˜Ÿåˆ—å¤§å°
        self.active_compressions = 0  # å½“å‰æ´»è·ƒå‹ç¼©æ•°
        
        # é€’å¢æ€»ç»“é…ç½®
        self.summary_layers = {
            'L1': {'max_turns': 2, 'description': 'å•è½®å¯¹è¯æ‘˜è¦'},  # æœ€è¿‘2è½®
            'L2': {'max_turns': 5, 'description': 'å¤šè½®å¯¹è¯æ‘˜è¦'},  # æœ€è¿‘5è½®
            'L3': {'max_turns': 10, 'description': 'ä¸»é¢˜èšç±»æ‘˜è¦'}   # æœ€è¿‘10è½®
        }
        
        # å‹ç¼©å¤„ç†å™¨å¯åŠ¨æ ‡å¿—
        self._processor_started = False
        
        app_logger.info("MemoryCompressor initialized")
    
    async def ensure_processor_started(self) -> None:
        """ç¡®ä¿å‹ç¼©å¤„ç†å™¨å·²å¯åŠ¨"""
        if not self._processor_started:
            self._processor_started = True
            asyncio.create_task(self._compression_processor())
            app_logger.info("Compression processor started")
    
    async def queue_compression_task(
        self,
        user_id: str,
        conversation_id: str,
        priority: str = 'normal'
    ) -> None:
        """å°†å‹ç¼©ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—"""
        try:
            task = {
                'id': str(uuid.uuid4()),
                'user_id': user_id,
                'conversation_id': conversation_id,
                'priority': priority,
                'created_at': datetime.now().isoformat(),
                'status': 'queued'
            }
            
            with self.compression_lock:
                # æ£€æŸ¥é˜Ÿåˆ—å¤§å°é™åˆ¶
                if len(self.compression_queue) >= self.max_queue_size:
                    # é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒæœ€è€çš„ä»»åŠ¡
                    if priority == 'high':
                        # é«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼Œä¸¢å¼ƒæœ€è€çš„æ™®é€šä¼˜å…ˆçº§ä»»åŠ¡
                        old_task = None
                        for i, queued_task in enumerate(self.compression_queue):
                            if queued_task.get('priority') == 'normal':
                                old_task = self.compression_queue.pop(i)
                                break
                        if old_task:
                            app_logger.warning(f"âš ï¸ [COMPRESS] Queue full, discarded normal priority task {old_task.get('id', 'unknown')}")
                        else:
                            app_logger.warning(f"âš ï¸ [COMPRESS] Queue full, cannot add high priority task {task['id']}")
                            return
                    else:
                        # æ™®é€šä¼˜å…ˆçº§ä»»åŠ¡ï¼Œä¸¢å¼ƒæœ€è€çš„ä»»åŠ¡
                        old_task = self.compression_queue.popleft()
                        app_logger.warning(f"âš ï¸ [COMPRESS] Queue full, discarded task {old_task.get('id', 'unknown')}")
                
                # æ·»åŠ æ–°ä»»åŠ¡
                if priority == 'high':
                    # é«˜ä¼˜å…ˆçº§ä»»åŠ¡æ’å…¥åˆ°é˜Ÿåˆ—å‰é¢
                    self.compression_queue.appendleft(task)
                else:
                    # æ™®é€šä¼˜å…ˆçº§ä»»åŠ¡æ’å…¥åˆ°é˜Ÿåˆ—åé¢
                    self.compression_queue.append(task)
            
            app_logger.info(f"ğŸ“¦ [COMPRESS] Queued compression task for {user_id}:{conversation_id} with priority {priority}")
            
        except Exception as e:
            app_logger.error(f"âŒ [COMPRESS] Failed to queue compression task: {e}")
    
    async def _compression_processor(self) -> None:
        """å¼‚æ­¥å‹ç¼©å¤„ç†å™¨"""
        while True:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒå‹ç¼©ä»»åŠ¡å’Œé˜Ÿåˆ—ä»»åŠ¡
                if self.active_compressions < self.max_concurrent_compressions and self.compression_queue:
                    # è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡
                    with self.compression_lock:
                        if not self.compression_queue:
                            continue
                        task = self.compression_queue.popleft()
                    
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    task['status'] = 'processing'
                    
                    # çº¿ç¨‹å®‰å…¨åœ°å¢åŠ æ´»è·ƒå‹ç¼©è®¡æ•°
                    with self.compression_lock:
                        self.active_compressions += 1
                    
                    # å¼‚æ­¥å¤„ç†å‹ç¼©ä»»åŠ¡
                    asyncio.create_task(self._process_compression_task(task))
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
                await asyncio.sleep(1)
                
            except Exception as e:
                app_logger.error(f"âŒ [COMPRESS] Compression processor error: {e}")
                await asyncio.sleep(5)
    
    async def _process_compression_task(self, task: Dict[str, Any]) -> None:
        """å¤„ç†å•ä¸ªå‹ç¼©ä»»åŠ¡"""
        try:
            user_id = task['user_id']
            conversation_id = task['conversation_id']
            task_id = task['id']
            
            app_logger.info(f"ğŸ”„ [COMPRESS] Processing compression task {task_id} for {user_id}:{conversation_id}")
            
            # è·å–å¯¹è¯æ¶ˆæ¯
            from database import conversation_repo
            all_messages = conversation_repo.get_current_conversation_messages(
                conversation_id=conversation_id,
                limit=100
            )
            
            if not all_messages:
                return
            
            # æ‰§è¡Œé€’å¢æ€»ç»“å‹ç¼©
            await self.incremental_compression(user_id, conversation_id, all_messages)
            
            app_logger.info(f"âœ… [COMPRESS] Completed compression task {task_id} for {user_id}:{conversation_id}")
            
        except Exception as e:
            app_logger.error(f"âŒ [COMPRESS] Failed to process compression task {task.get('id', 'unknown')}: {e}")
        finally:
            # çº¿ç¨‹å®‰å…¨åœ°å‡å°‘æ´»è·ƒå‹ç¼©è®¡æ•°
            with self.compression_lock:
                self.active_compressions = max(0, self.active_compressions - 1)
    
    async def incremental_compression(
        self,
        user_id: str,
        conversation_id: str,
        messages: List[Dict[str, Any]]
    ) -> bool:
        """é€’å¢æ€»ç»“å‹ç¼©"""
        try:
            total_messages = len(messages)
            if total_messages < 6:  # å°‘äº3è½®å¯¹è¯ä¸éœ€è¦å‹ç¼©
                return True
            
            # è·å–ç°æœ‰çš„æ‘˜è¦å±‚
            existing_summaries = await self._get_existing_summaries(user_id, conversation_id)
            
            # åˆ†å±‚å¤„ç† - æŒ‰å±‚çº§ä»å¤§åˆ°å°å¤„ç†
            new_summaries = {}
            messages_to_keep = []
            
            # ç¡®å®šéœ€è¦ä¿ç•™çš„æ¶ˆæ¯æ•°é‡ï¼ˆå–æœ€å¤§çš„å±‚ï¼‰
            max_keep_turns = max(
                self.summary_layers['L1']['max_turns'],
                self.summary_layers['L2']['max_turns'], 
                self.summary_layers['L3']['max_turns']
            )
            
            # ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
            if total_messages > max_keep_turns:
                messages_to_keep = messages[-max_keep_turns:]
                messages_to_summarize = messages[:-max_keep_turns]
            else:
                messages_to_keep = messages
                messages_to_summarize = []
            
            # å¦‚æœæ²¡æœ‰éœ€è¦æ‘˜è¦çš„æ¶ˆæ¯ï¼Œç›´æ¥è¿”å›
            if not messages_to_summarize:
                app_logger.info(f"â„¹ï¸ [COMPRESS] No messages need summarization for {user_id}:{conversation_id}")
                return True
            
            # ç”Ÿæˆå„å±‚æ‘˜è¦
            previous_summary = ""
            for layer in ['L1', 'L2', 'L3']:
                layer_summary = await summary_generator.generate_layer_summary(
                    layer=layer,
                    messages=messages_to_summarize,
                    previous_summary=previous_summary
                )
                
                if layer_summary:
                    new_summaries[layer] = layer_summary
                    previous_summary = layer_summary
            
            # å­˜å‚¨å„å±‚æ‘˜è¦
            if new_summaries:
                await self._store_layer_summaries(user_id, conversation_id, new_summaries)
            
            # æ¸…ç†æ—§æ¶ˆæ¯ï¼ˆåªä¿ç•™æœ€è¿‘çš„ï¼‰
            await self._cleanup_old_messages(user_id, conversation_id, messages_to_keep)
            
            app_logger.info(f"âœ¨ [COMPRESS] Incremental compression completed for {user_id}:{conversation_id}")
            return True
            
        except Exception as e:
            app_logger.error(f"âŒ [COMPRESS] Incremental compression failed: {e}")
            return False
    
    async def _get_existing_summaries(
        self,
        user_id: str,
        conversation_id: str
    ) -> Dict[str, str]:
        """è·å–ç°æœ‰çš„æ‘˜è¦å±‚"""
        try:
            summaries = {}
            for layer in ['L1', 'L2', 'L3']:
                summary = await redis_manager.get_conversation_summary(
                    user_id, conversation_id, layer
                )
                if summary:
                    summaries[layer] = summary
            return summaries
        except Exception as e:
            app_logger.error(f"âŒ [COMPRESS] Failed to get existing summaries: {e}")
            return {}
    
    async def _store_layer_summaries(
        self,
        user_id: str,
        conversation_id: str,
        summaries: Dict[str, str]
    ) -> None:
        """å­˜å‚¨å„å±‚æ‘˜è¦"""
        try:
            for layer, summary in summaries.items():
                await redis_manager.set_conversation_summary(
                    user_id=user_id,
                    conversation_id=conversation_id,
                    layer=layer,
                    summary=summary
                )
            app_logger.info(f"ğŸ’¾ [COMPRESS] Stored {len(summaries)} layer summaries for {user_id}:{conversation_id}")
        except Exception as e:
            app_logger.error(f"âŒ [COMPRESS] Failed to store layer summaries: {e}")
    
    async def _cleanup_old_messages(
        self,
        user_id: str,
        conversation_id: str,
        messages_to_keep: List[Dict[str, Any]]
    ) -> None:
        """æ¸…ç†æ—§æ¶ˆæ¯ï¼Œåªä¿ç•™æŒ‡å®šçš„æ¶ˆæ¯"""
        try:
            # ä»Redisä¸­æ¸…é™¤æ—§æ¶ˆæ¯ï¼Œåªä¿ç•™æœ€è¿‘çš„
            from database import conversation_repo
            
            # è·å–æ‰€æœ‰æ¶ˆæ¯ID
            all_message_ids = {msg.get('id') for msg in messages_to_keep if msg.get('id')}
            
            # è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ¶ˆæ¯
            db_messages = conversation_repo.get_current_conversation_messages(
                conversation_id=conversation_id,
                limit=100
            )
            
            # åˆ é™¤ä¸åœ¨ä¿ç•™åˆ—è¡¨ä¸­çš„æ¶ˆæ¯
            deleted_count = 0
            for msg in db_messages:
                msg_id = msg.get('id')
                if msg_id and msg_id not in all_message_ids:
                    # ä»Redisä¸­åˆ é™¤
                    try:
                        conv_id = f"{user_id}:{conversation_id}:{msg_id}"
                        await redis_manager.delete_conversation_message(
                            user_id, conversation_id, conv_id
                        )
                        deleted_count += 1
                    except Exception as delete_error:
                        app_logger.warning(f"âš ï¸ [COMPRESS] Failed to delete message {conv_id}: {delete_error}")
            
            app_logger.info(f"ğŸ—‘ï¸ [COMPRESS] Cleanup completed for {user_id}:{conversation_id} - kept {len(messages_to_keep)} messages, deleted {deleted_count} old messages")
            
        except Exception as e:
            app_logger.error(f"âŒ [COMPRESS] Failed to cleanup old messages: {e}")


# å…¨å±€å®ä¾‹
memory_compressor = MemoryCompressor()

